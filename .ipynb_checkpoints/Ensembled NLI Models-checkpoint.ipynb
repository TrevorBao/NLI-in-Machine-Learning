{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89103c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in d:\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in d:\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.39.1)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in d:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in d:\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.22.0)\n",
      "Requirement already satisfied: Pillow in d:\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682da3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45cff7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('train.csv')\n",
    "validation_data = pd.read_csv('dev.csv')\n",
    "\n",
    "# Load the SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a830a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_embeddings(dataframe, model):\n",
    "#     # Generate embeddings for premises and hypotheses separately\n",
    "#     premise_embeddings = model.encode(dataframe['premise'].tolist(), show_progress_bar=True)\n",
    "#     hypothesis_embeddings = model.encode(dataframe['hypothesis'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "#     # Combine the two sets of embeddings into one feature set\n",
    "#     combined_embeddings = np.concatenate((premise_embeddings, hypothesis_embeddings), axis=1)\n",
    "    \n",
    "#     return combined_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f027458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sentence embeddings\n",
    "def generate_embeddings(dataframe, model):\n",
    "    premises = dataframe['premise'].astype(str).tolist()\n",
    "    hypotheses = dataframe['hypothesis'].astype(str).tolist()\n",
    "    sentence_pairs = [premise + \" \" + hypothesis for premise, hypothesis in zip(premises, hypotheses)]\n",
    "    embeddings = model.encode(sentence_pairs, show_progress_bar=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0266c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df53c16ac284bc4bf4bf7ce336a2cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/842 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418b28805829491f81e0406f44769688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings for training and validation data\n",
    "train_embeddings = generate_embeddings(data, model)\n",
    "validation_embeddings = generate_embeddings(validation_data, model)\n",
    "\n",
    "# Prepare the labels\n",
    "train_labels = data['label'].values\n",
    "validation_labels = validation_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d789399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base learners\n",
    "base_learners = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42, verbose=1)),\n",
    "    ('svm', make_pipeline(StandardScaler(), SVC(probability=True, verbose=True))),\n",
    "    ('logreg', make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, solver='saga', random_state=42, verbose=1)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1edada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Stacking Classifier\n",
    "stacking_classifier = StackingClassifier(estimators=base_learners, final_estimator=GradientBoostingClassifier(random_state=42, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec86b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Stacking Classifier\n",
    "stacking_classifier.fit(train_embeddings, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained stacking classifier to a file\n",
    "model_filename = 'ensembleNLI2.joblib'\n",
    "joblib.dump(stacking_classifier, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defae0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Stacking Classifier on the validation data\n",
    "validation_predictions = stacking_classifier.predict(validation_embeddings)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "report = classification_report(validation_labels, validation_predictions)\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(validation_predictions, columns=['prediction'])\n",
    "predictions_df.to_csv('validation_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd054158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
